# -*- coding: utf-8 -*-
"""Aniket_Sunil_Shinde_NLP_Test_Task

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/aniket-sunil-shinde-nlp-test-task-11fba4eb-a7c8-4d0c-9072-af425b316cb8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240429/auto/storage/goog4_request%26X-Goog-Date%3D20240429T133031Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9ef2e2a85123df72bcfcf3c0d67f1767db56730fc28cb6e2a44b1b4df3466c74ddff2c06928ccf65667cbe1d40b3fd03b967d9f81f84422ff5c4e3d5adfba0b1c9ee71b4e9acd7f8f2d8043c1b71053e2afa3e4a6501514928db33692ed03a8294a30feb486f32838bfe453de13a7b3b927974ac80d4a842cef459bc47356d8819a340ca5ff06942c7edd3dcba2876ea4772f688c0ad82650e2ff8972610b8db827a884e6aea6c0498f05aa0378b3d8b2b0001f4327cf894330b16f459f4153c11b6c2bf3d15dd2cd3d36420b76c428051cb646d15218924da93ba7e871dd99491b5cc83aeec3b3b84a22bb493c7f64b741c8085aa7a506f276bde3ec5749d6b

# Install the required libraries
"""

!pip3 install transformers
!pip3 install accelerate -U
!pip3 install datasets

"""# Initialize BERT and initialize working in GPU environment"""

import torch
model_name = "bert-base-uncased"
max_length = 512
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device: ",device)

from transformers import BertTokenizerFast,BertForSequenceClassification
tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)

"""# Import dataset and create training and testing datasets"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split

dataset = fetch_20newsgroups(subset="all", shuffle=True, remove=("headers","footers", "quotes"))
target_names=dataset.target_names
news_text = dataset.data
labels = dataset.target
print("The Twenty newsgroups",target_names)
print("Sample Text in the dataset\n", news_text[0])
print("The NewsGroup for the sample text is", target_names[labels[0]])
(train_texts,valid_texts,train_labels,valid_labels)=train_test_split(news_text, labels, test_size=0.3, random_state = 42)

"""# Create BERT Embeddings for the textual data and create modified datasets for BERT Training"""

import torch
from datasets import Dataset
# Tokenize and encode test data

train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=max_length, return_tensors="pt")
test_encodings = tokenizer(valid_texts, truncation=True, padding='max_length', max_length=max_length, return_tensors="pt")

# Convert labels to tensors
train_labels_tensor = torch.tensor(train_labels,dtype=torch.long)  # Example labels for training data
test_labels_tensor = torch.tensor(valid_labels,dtype=torch.long)   # Example labels for test data

# Create a Hugging Face Dataset object
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels':  torch.tensor(train_labels) # Assuming y_train contains the labels
})

test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels':  torch.tensor(valid_labels) # Assuming y_train contains the labels
})

"""# Creating Evaluation metrics for analyzing the training progress and also for testing on test data finally"""

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

"""# Loading the pretrained BERT Model and setting the training arguments for classification training"""

model=BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names))
from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir = '/content/drive/MyDrive/NLP/Model_Outputs/',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    # batch size per device during training
    weight_decay=0.01,               # strength of weight decay
    load_best_model_at_end=True,
    evaluation_strategy="steps",
)

"""# Start the training process by providing training and evaluation datasets and the necessary training arguments and metrics"""

trainer = Trainer( model=model,args=training_args,compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=test_dataset)
trainer.train()

"""# Perform evaluation on the test dataset"""

#Evaluate the model on the test dataset
eval_results = trainer.evaluate(test_dataset)

# Calculate accuracy and F1 score
accuracy = eval_results['eval_accuracy']
f1 = eval_results['eval_f1']

#Printing the Final Result on Test Data
print("Final Evaluation Loss", eval_results['eval_loss'])
print("Accuracy", accuracy)
print("F1 Score:", f1)